{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "get_ipython().system(u'pip install nilearn')\n",
    "get_ipython().system(u'pip install tf-explain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vasan\\Downloads\\Programs\\Python\\Python310\\lib\\site-packages\\nilearn\\input_data\\__init__.py:23: FutureWarning: The import path 'nilearn.input_data' is deprecated in version 0.9. Importing from 'nilearn.input_data' will be possible at least until release 0.13.0. Please import from 'nilearn.maskers' instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "c:\\Users\\vasan\\Downloads\\Programs\\Python\\Python310\\lib\\site-packages\\nilearn\\datasets\\struct.py:688: UserWarning: Only 403 subjects are available in the DARTEL-normalized version of the dataset. All of them will be used instead of the wanted 416\n",
      "  warnings.warn(\n",
      "c:\\Users\\vasan\\Downloads\\Programs\\Python\\Python310\\lib\\site-packages\\nilearn\\datasets\\struct.py:852: UserWarning: `legacy_format` will default to `False` in release 0.11. Dataset fetchers will then return pandas dataframes by default instead of recarrays.\n",
      "  warnings.warn(_LEGACY_FORMAT_MSG)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import linalg\n",
    "\n",
    "from nilearn import datasets\n",
    "from nilearn.input_data import NiftiMasker\n",
    "\n",
    "from nilearn.image import smooth_img\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import keras\n",
    "\n",
    "n_subjects = 416\n",
    "\n",
    "oasis_dataset = datasets.fetch_oasis_vbm(n_subjects=n_subjects)\n",
    "gray_matter_map_filenames = oasis_dataset.gray_matter_maps\n",
    "gm_imgs = gray_matter_map_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdr = oasis_dataset.ext_vars['cdr'].astype(float)\n",
    "cdr_numpy_arr = np.array(cdr)\n",
    "for i in range(len(cdr_numpy_arr)):\n",
    "    if(np.isnan(cdr_numpy_arr[i])): \n",
    "        cdr_numpy_arr[i] = 1\n",
    "    elif(cdr_numpy_arr[i] > 0.0):\n",
    "        cdr_numpy_arr[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgArr = []\n",
    "\n",
    "for imgUrl in gray_matter_map_filenames:\n",
    "    result_img = smooth_img(imgUrl, fwhm=1)\n",
    "    imgArr.append(result_img.get_fdata())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "x_test = []\n",
    "\n",
    "y_train = []\n",
    "y_test = []\n",
    "\n",
    "rshapedImgArr = []\n",
    "\n",
    "for img in imgArr:\n",
    "    newImg = [cv2.resize(each_slice,(50,50)) for each_slice in img]#Reducing slice count\n",
    "    newImg = np.array(newImg)\n",
    "    rshapedImgArr.append(newImg)\n",
    "    \n",
    "label = cdr_numpy_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = keras.utils.to_categorical(cdr_numpy_arr, 2)\n",
    "\n",
    "much_data = []\n",
    "\n",
    "for num, img in enumerate(rshapedImgArr):\n",
    "    much_data.append([img,label[num]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "\n",
    "IMG_SIZE_PX_X = 50\n",
    "IMG_SIZE_PX_Y = 50\n",
    "SLICE_COUNT = 91\n",
    "\n",
    "n_classes = 2\n",
    "batch_size = 10\n",
    "\n",
    "x = tf.placeholder('float')\n",
    "y = tf.placeholder('float')\n",
    "\n",
    "keep_rate = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3d(x, W):\n",
    "    conv = tf.nn.conv3d(x, W, strides=[1,1,1,1,1], padding='SAME')\n",
    "    conv = tf.nn.dropout(conv, 0.5)\n",
    "    return conv\n",
    "\n",
    "def maxpool3d(x):\n",
    "    #                        size of window         movement of window as you slide about\n",
    "    return tf.nn.max_pool3d(x, ksize=[1,2,2,2,1], strides=[1,2,2,2,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_neural_network(x):\n",
    "    #                # 5 x 5 x 5 patches, 1 channel, 32 features to compute.\n",
    "    weights = {'W_conv1':tf.Variable(tf.random_normal([3,3,3,1,32])),\n",
    "                #       5 x 5 x 5 patches, 32 channels, 64 features to compute.\n",
    "                'W_conv2':tf.Variable(tf.random_normal([3,3,3,32,64])),\n",
    "                #                                  64 features\n",
    "                'W_fc':tf.Variable(tf.random_normal([248768,1024])),\n",
    "                'out':tf.Variable(tf.random_normal([1024, n_classes]))}\n",
    "\n",
    "    biases = {'b_conv1':tf.Variable(tf.random_normal([32])),\n",
    "                'b_conv2':tf.Variable(tf.random_normal([64])),\n",
    "                'b_fc':tf.Variable(tf.random_normal([1024])),\n",
    "                'out':tf.Variable(tf.random_normal([n_classes]))}\n",
    "\n",
    "    #                            image X      image Y        image Z\n",
    "    x = tf.reshape(x, shape=[-1, IMG_SIZE_PX_X, IMG_SIZE_PX_Y, SLICE_COUNT, 1])\n",
    "\n",
    "    conv1 = tf.nn.relu(conv3d(x, weights['W_conv1']) + biases['b_conv1'])\n",
    "    conv1 = maxpool3d(conv1)\n",
    "\n",
    "\n",
    "    conv2 = tf.nn.relu(conv3d(conv1, weights['W_conv2']) + biases['b_conv2'])\n",
    "    conv2 = maxpool3d(conv2)\n",
    "\n",
    "    fc = tf.reshape(conv2,[-1, 248768])\n",
    "    fc = tf.nn.relu(tf.matmul(fc, weights['W_fc'])+biases['b_fc'])\n",
    "    fc = tf.nn.dropout(fc, keep_rate)\n",
    "\n",
    "    output = tf.matmul(fc, weights['out'])+biases['out']\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = much_data[:-333]\n",
    "# validation_data = much_data[-83:]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train_neural_network(x):\n",
    "    prediction = convolutional_neural_network(x)\n",
    "    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y) )\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(cost)\n",
    "    \n",
    "    file = open(\"Output.txt\", \"a+\")\n",
    "    \n",
    "    # hm_epochs = 1000\n",
    "    hm_epochs = 50\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        \n",
    "        successful_runs = 0\n",
    "        total_runs = 0\n",
    "        \n",
    "        for epoch in range(hm_epochs):\n",
    "            epoch_loss = 0\n",
    "            \n",
    "            train_data, validation_data = train_test_split(much_data, train_size=0.8)\n",
    "            \n",
    "            for data in train_data:\n",
    "                total_runs += 1\n",
    "                try:\n",
    "                    X = data[0]\n",
    "                    Y = data[1]\n",
    "                    _, c = sess.run([optimizer, cost], feed_dict={x: X, y: Y})\n",
    "                    epoch_loss += c\n",
    "                    successful_runs += 1\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "                    #print(str(e))\n",
    "            \n",
    "            print(f\"Epoch {epoch+1} completed out of {hm_epochs} loss: {epoch_loss}\")\n",
    "            file.write(f\"Epoch {epoch+1} completed out of {hm_epochs} loss: {epoch_loss}\")\n",
    "\n",
    "            correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "\n",
    "            print(f\"Accuracy: {accuracy.eval({x:[i[0] for i in validation_data], y:[i[1] for i in validation_data]})}\")\n",
    "            file.write(f\"Accuracy: {accuracy.eval({x:[i[0] for i in validation_data], y:[i[1] for i in validation_data]})}\")\n",
    "            \n",
    "            save_path = saver.save(sess, \"model.ckpt\")\n",
    "            print(f\"Model saved in file: {save_path}\")\n",
    "\n",
    "        print('Done. Finishing accuracy: ')\n",
    "        print(f\"Accuracy: {accuracy.eval({x:[i[0] for i in validation_data], y:[i[1] for i in validation_data]})}\")\n",
    "        \n",
    "        print(f\"Fitment percent: {successful_runs/total_runs}\")\n",
    "        \n",
    "        file.write(f\"Done. Finishing accuracy: \")\n",
    "        file.write(f\"Accuracy: {accuracy.eval({x:[i[0] for i in validation_data], y:[i[1] for i in validation_data]})}\")\n",
    "        \n",
    "        file.write(f\"Fitment percent: {successful_runs/total_runs}\")\n",
    "\n",
    "        save_path = saver.save(sess, \"model.ckpt\")\n",
    "        print(f\"Final model saved in file: {save_path}\")\n",
    "\n",
    "train_neural_network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guided_backpropagation(guided_model, input_img_data):\n",
    "    input_imgs = input_img_data.reshape((1, IMG_SIZE_PX_X, IMG_SIZE_PX_Y, SLICE_COUNT, 1))\n",
    "    layer_idx = [idx for idx, layer in enumerate(guided_model.layers) if \"conv3d\" in layer.name][-1]\n",
    "    grads = K.gradients(guided_model.output, guided_model.layers[layer_idx].output)[0]\n",
    "    backprop_fn = K.function([guided_model.input], [grads])\n",
    "    guided_grads = backprop_fn([input_imgs])[0]\n",
    "    guided_grads = guided_grads[0, :, :, :, :]\n",
    "    return guided_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_guided_backpropagation(model, input_img_data):\n",
    "    guided_model = tf.keras.models.Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "    guided_backprop = guided_backpropagation(guided_model, input_img_data)\n",
    "    return guided_backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = tf.keras.models.load_model(\"model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = convolutional_neural_network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an example from the test set for visualization\n",
    "example = much_data[0]\n",
    "input_img_data = example[0]  # Replace this with the input image you want to visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "# Grad-CAM visualization\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    saver.restore(sess, \"model.ckpt\")\n",
    "\n",
    "    # Get the output tensor of the model\n",
    "    output_tensor = model.output\n",
    "\n",
    "    # Get the convolutional layer for visualization\n",
    "    last_conv_layer = model.get_layer(\"conv2\")  # Change this to the name of your last convolutional layer\n",
    "\n",
    "    # Compute the gradient of the predicted class with respect to the output feature map of the last conv layer\n",
    "    grads = K.gradients(output_tensor, last_conv_layer.output)[0]\n",
    "\n",
    "    # Vectorized function to get the values of the gradients and the last conv layer output\n",
    "    gradient_function = K.function([model.input], [last_conv_layer.output, grads])\n",
    "\n",
    "    # Get the output and gradients\n",
    "    output, grads_val = gradient_function([input_img_data.reshape((1, IMG_SIZE_PX_X, IMG_SIZE_PX_Y, SLICE_COUNT, 1))])\n",
    "\n",
    "    # Compute the mean intensity of the gradients over each feature map\n",
    "    pooled_grads_val = np.mean(grads_val, axis=(0, 1, 2, 3))\n",
    "\n",
    "    # Multiply each channel in the feature map array by \"how important this channel is\" with regard to the predicted class\n",
    "    for i in range(last_conv_layer.output_shape[4]):\n",
    "        output[:, :, :, i] *= pooled_grads_val[i]\n",
    "\n",
    "    # Average the feature map along the axis 3 (number of channels)\n",
    "    heatmap = np.mean(output, axis=-1)\n",
    "    heatmap = np.maximum(heatmap, 0)\n",
    "    heatmap /= np.max(heatmap)\n",
    "\n",
    "    # Upsample the heatmap to match the size of the input image\n",
    "    heatmap = cv2.resize(heatmap[0, :, :, :], (IMG_SIZE_PX_Y, IMG_SIZE_PX_X))\n",
    "\n",
    "    # Convert the heatmap to RGB\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "\n",
    "    # Superimpose the heatmap on the original image\n",
    "    superimposed_img = cv2.addWeighted(input_img_data[:, :, :, 0], 0.6, heatmap, 0.4, 0)\n",
    "\n",
    "    # Display the original image, heatmap, and superimposed image\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(input_img_data[:, :, :, 0], cmap='gray')\n",
    "    plt.title('Original Image')\n",
    "\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(heatmap, cmap='jet')\n",
    "    plt.title('Grad-CAM Heatmap')\n",
    "\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(superimposed_img, cmap='jet')\n",
    "    plt.title('Superimposed Image')\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
