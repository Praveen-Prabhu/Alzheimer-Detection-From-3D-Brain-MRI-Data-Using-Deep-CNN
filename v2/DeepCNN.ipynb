{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "get_ipython().system(u'pip install nilearn')\n",
    "get_ipython().system(u'pip install tf-explain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from nilearn import datasets  # Add this line\n",
    "from nilearn.image import smooth_img\n",
    "from tf_explain.core import GradCAM\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch dataset\n",
    "n_subjects = 416\n",
    "oasis_dataset = datasets.fetch_oasis_vbm(n_subjects=n_subjects)\n",
    "gray_matter_map_filenames = oasis_dataset.gray_matter_maps\n",
    "\n",
    "cdr = oasis_dataset.ext_vars['cdr'].astype(float)\n",
    "cdr_numpy_arr = np.array(cdr)\n",
    "cdr_numpy_arr[np.isnan(cdr_numpy_arr)] = 1\n",
    "cdr_numpy_arr[cdr_numpy_arr > 0.0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgArr = []\n",
    "for imgUrl in gray_matter_map_filenames:\n",
    "    result_img = smooth_img(imgUrl, fwhm=1)\n",
    "    imgArr.append(result_img.get_fdata())\n",
    "\n",
    "rshapedImgArr = []\n",
    "for img in imgArr:\n",
    "    newImg = [cv2.resize(each_slice, (50, 50)) for each_slice in img]  # Reducing slice count\n",
    "    newImg = np.array(newImg)\n",
    "    rshapedImgArr.append(newImg)\n",
    "\n",
    "label = to_categorical(cdr_numpy_arr, 2)\n",
    "\n",
    "much_data = []\n",
    "for num, img in enumerate(rshapedImgArr):\n",
    "    much_data.append([img, label[num]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the convolutional neural network model\n",
    "IMG_SIZE_PX_X = 50\n",
    "IMG_SIZE_PX_Y = 50\n",
    "SLICE_COUNT = 91\n",
    "n_classes = 2\n",
    "batch_size = 10\n",
    "\n",
    "model = Sequential([\n",
    "    Conv3D(32, (3, 3, 3), input_shape=(IMG_SIZE_PX_X, IMG_SIZE_PX_Y, SLICE_COUNT, 1), activation='relu', padding='same'),\n",
    "    MaxPooling3D(pool_size=(2, 2, 2), padding='same'),\n",
    "    Conv3D(64, (3, 3, 3), activation='relu', padding='same'),\n",
    "    MaxPooling3D(pool_size=(2, 2, 2), padding='same'),\n",
    "    Flatten(),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dropout(0.8),\n",
    "    Dense(n_classes, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "train_data, validation_data = train_test_split(much_data, train_size=0.8)\n",
    "\n",
    "# Convert the data to NumPy arrays\n",
    "X_train = np.array([i[0] for i in train_data])\n",
    "Y_train = np.array([i[1] for i in train_data])\n",
    "X_val = np.array([i[0] for i in validation_data])\n",
    "Y_val = np.array([i[1] for i in validation_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=50, batch_size=batch_size)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"model.h5\")\n",
    "\n",
    "# Save history to a pickle file\n",
    "with open(\"history.pkl\", \"wb\") as file:\n",
    "    pickle.dump(history, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try loading history from the pickle file\n",
    "try:\n",
    "    with open(\"history.pkl\", \"rb\") as file:\n",
    "        history = pickle.load(file)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Write epoch-wise history to \"Output.txt\" file\n",
    "with open(\"Output.txt\", \"w\") as file:\n",
    "    file.write(\"Epoch\\tLoss\\tAccuracy\\n\")\n",
    "    for epoch in range(1, 51):  # Assuming 50 epochs\n",
    "        loss = history.history['loss'][epoch-1]\n",
    "        accuracy = history.history['accuracy'][epoch-1]\n",
    "        val_loss = history.history['val_loss'][epoch-1]\n",
    "        val_accuracy = history.history['val_accuracy'][epoch-1]\n",
    "\n",
    "        file.write(f\"{epoch}\\t{loss}\\t{accuracy}\\t{val_loss}\\t{val_accuracy}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting epoch-wise accuracy\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Epoch-wise Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plotting epoch-wise loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Epoch-wise Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grad-CAM Visualization\n",
    "index = 0  # Choose an index from the validation set\n",
    "image = X_val[index][np.newaxis, ...]\n",
    "class_index = np.argmax(Y_val[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try loading the saved model\n",
    "try:\n",
    "    model = tf.keras.models.load_model(\"model.h5\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create Grad-CAM explainer\n",
    "explainer = Gradcam()\n",
    "\n",
    "# Apply explainer to the model\n",
    "grid = explainer.explain(([image], None), model, class_index=class_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the original image and Grad-CAM heatmap\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image[0, :, :, SLICE_COUNT // 2, 0], cmap='gray')\n",
    "plt.title('Original Image')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(grid[0], cmap='jet', alpha=0.8)\n",
    "plt.imshow(image[0, :, :, SLICE_COUNT // 2, 0], cmap='gray', alpha=0.2)\n",
    "plt.title('Grad-CAM Heatmap')\n",
    "\n",
    "# Save the superimposed image\n",
    "plt.savefig('gradcam_superimposed_image.png')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
